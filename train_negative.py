#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¯æŒè´Ÿæ ·æœ¬çš„YOLO11/12è®­ç»ƒç¨‹åº
ä¸“é—¨ç”¨äºç«ç‚¹æ£€æµ‹ï¼Œæ”¯æŒç©ºæ ‡ç­¾æ–‡ä»¶ï¼ˆè´Ÿæ ·æœ¬ï¼‰çš„è®­ç»ƒ
"""

import os
import sys
import yaml
import argparse
import logging
from pathlib import Path
from typing import Dict, Any, List
import shutil
import random


def setup_interrupt_handlers():
    """è®¾ç½®ä¸­æ–­å¤„ç†å™¨ï¼Œç¡®ä¿DDPè®­ç»ƒæ—¶èƒ½æ­£ç¡®ç»ˆæ­¢"""
    import signal
    import subprocess

    def emergency_cleanup():
        """ç´§æ€¥æ¸…ç†å‡½æ•°"""
        print("\nğŸš¨ ç´§æ€¥ç»ˆæ­¢è®­ç»ƒè¿›ç¨‹...")
        cleanup_commands = [
            ['pkill', '-f', 'Ultralytics'],
            ['pkill', '-f', '_temp_'],
            ['pkill', '-f', 'yolo'],
            ['pkill', '-9', '-f', 'python']
        ]

        for cmd in cleanup_commands:
            try:
                subprocess.run(cmd, timeout=3, capture_output=True)
            except:
                pass
        print("âœ… æ¸…ç†å®Œæˆ")

    return emergency_cleanup

try:
    from ultralytics import YOLO
    from ultralytics.nn.tasks import DetectionModel
except ImportError:
    print("è¯·å…ˆå®‰è£…ultralyticsåº“: pip install ultralytics")
    exit(1)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class YOLONegativeTrainer:
    """æ”¯æŒè´Ÿæ ·æœ¬çš„YOLO11/12è®­ç»ƒå™¨ç±»"""

    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self.load_config(config_path)
        self.validate_config()

        # åŠ è½½æ•°æ®é›†é…ç½®
        self.dataset_config = self.load_dataset_config()
        self.num_classes = self.dataset_config.get('nc', 0)

        logger.info(f"æ£€æµ‹åˆ° {self.num_classes} ä¸ªç±»åˆ«: {self.dataset_config.get('names', [])}")

    def load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½è®­ç»ƒé…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            return config
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise

    def load_dataset_config(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é›†é…ç½®æ–‡ä»¶"""
        dataset_yaml_path = self.config['dataset']['dataset_yaml']
        try:
            with open(dataset_yaml_path, 'r', encoding='utf-8') as f:
                dataset_config = yaml.safe_load(f)
            logger.info(f"æˆåŠŸåŠ è½½æ•°æ®é›†é…ç½®: {dataset_yaml_path}")
            return dataset_config
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®é›†é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise

    def validate_config(self):
        """éªŒè¯é…ç½®æ–‡ä»¶çš„æœ‰æ•ˆæ€§"""
        required_keys = ['model', 'training', 'dataset']
        for key in required_keys:
            if key not in self.config:
                raise ValueError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…è¦å­—æ®µ: {key}")

        # éªŒè¯æ¨¡å‹ç±»å‹
        valid_models = [
            'yolo11n', 'yolo11s', 'yolo11m', 'yolo11l', 'yolo11x',
            'yolo12n', 'yolo12s', 'yolo12m', 'yolo12l', 'yolo12x'
        ]
        model_name = self.config['model']['name']
        if model_name not in valid_models:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_name}, æ”¯æŒçš„ç±»å‹: {valid_models}")

        logger.info("é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡")

    def validate_dataset_structure(self):
        """éªŒè¯æ•°æ®é›†ç»“æ„ï¼Œå¤„ç†è´Ÿæ ·æœ¬"""
        dataset_root = Path(self.config['dataset']['root_path'])

        train_images_dir = dataset_root / 'images' / 'train'
        train_labels_dir = dataset_root / 'labels' / 'train'
        val_images_dir = dataset_root / 'images' / 'val'
        val_labels_dir = dataset_root / 'labels' / 'val'

        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        for dir_path in [train_images_dir, train_labels_dir, val_images_dir, val_labels_dir]:
            if not dir_path.exists():
                logger.warning(f"ç›®å½•ä¸å­˜åœ¨: {dir_path}")

        # ç»Ÿè®¡è´Ÿæ ·æœ¬æ•°é‡
        negative_train_count = self._count_negative_samples(train_labels_dir)
        negative_val_count = self._count_negative_samples(val_labels_dir)

        logger.info(f"è®­ç»ƒé›†è´Ÿæ ·æœ¬æ•°é‡: {negative_train_count}")
        logger.info(f"éªŒè¯é›†è´Ÿæ ·æœ¬æ•°é‡: {negative_val_count}")

        # æ£€æŸ¥å›¾åƒå’Œæ ‡ç­¾å¯¹åº”å…³ç³»
        self._check_image_label_consistency(train_images_dir, train_labels_dir, "è®­ç»ƒé›†")
        self._check_image_label_consistency(val_images_dir, val_labels_dir, "éªŒè¯é›†")

    def _count_negative_samples(self, labels_dir: Path) -> int:
        """ç»Ÿè®¡è´Ÿæ ·æœ¬æ•°é‡ï¼ˆç©ºæ ‡ç­¾æ–‡ä»¶ï¼‰"""
        if not labels_dir.exists():
            return 0

        negative_count = 0
        for label_file in labels_dir.glob('*.txt'):
            if label_file.stat().st_size == 0:
                negative_count += 1

        return negative_count

    def _check_image_label_consistency(self, images_dir: Path, labels_dir: Path, split_name: str):
        """æ£€æŸ¥å›¾åƒå’Œæ ‡ç­¾æ–‡ä»¶çš„ä¸€è‡´æ€§"""
        if not images_dir.exists() or not labels_dir.exists():
            logger.warning(f"{split_name}å›¾åƒæˆ–æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨")
            return

        # è·å–å›¾åƒå’Œæ ‡ç­¾æ–‡ä»¶é›†åˆ
        image_files = {f.stem for f in images_dir.glob('*') if f.suffix.lower() in ['.jpg', '.jpeg', '.png']}
        label_files = {f.stem for f in labels_dir.glob('*.txt')}

        # æ£€æŸ¥ç¼ºå¤±çš„æ ‡ç­¾æ–‡ä»¶ï¼ˆè´Ÿæ ·æœ¬ï¼‰
        missing_labels = image_files - label_files
        if missing_labels:
            logger.info(f"{split_name}ä¸­å‘ç°{len(missing_labels)}ä¸ªå›¾åƒæ²¡æœ‰å¯¹åº”æ ‡ç­¾æ–‡ä»¶ï¼ˆå°†ä½œä¸ºè´Ÿæ ·æœ¬å¤„ç†ï¼‰")
            # ä¸ºè¿™äº›å›¾åƒåˆ›å»ºç©ºçš„æ ‡ç­¾æ–‡ä»¶
            for img_name in missing_labels:
                empty_label_path = labels_dir / f"{img_name}.txt"
                empty_label_path.touch()
                logger.debug(f"åˆ›å»ºç©ºæ ‡ç­¾æ–‡ä»¶: {empty_label_path}")

        # æ£€æŸ¥å¤šä½™çš„æ ‡ç­¾æ–‡ä»¶
        extra_labels = label_files - image_files
        if extra_labels:
            logger.warning(f"{split_name}ä¸­å‘ç°{len(extra_labels)}ä¸ªæ ‡ç­¾æ–‡ä»¶æ²¡æœ‰å¯¹åº”çš„å›¾åƒ")

        logger.info(f"{split_name}ç»Ÿè®¡:")
        logger.info(f"  å›¾åƒæ–‡ä»¶: {len(image_files)}")
        logger.info(f"  æ ‡ç­¾æ–‡ä»¶: {len(label_files)}")
        logger.info(f"  æœ‰æ•ˆæ ·æœ¬: {len(image_files)}")

    def get_model_name(self) -> str:
        """è·å–å®Œæ•´çš„æ¨¡å‹åç§°"""
        model_config = self.config['model']
        base_name = model_config['name']

        # å¦‚æœæœ‰é¢„è®­ç»ƒæƒé‡ï¼Œä½¿ç”¨é¢„è®­ç»ƒæƒé‡
        if 'pretrained' in model_config and model_config['pretrained']:
            return model_config['pretrained']

        return f"{base_name}.pt"

    def setup_training_args(self) -> Dict[str, Any]:
        """è®¾ç½®è®­ç»ƒå‚æ•°"""
        training_config = self.config['training']

        # åŸºç¡€è®­ç»ƒå‚æ•° - é’ˆå¯¹ç«ç‚¹æ£€æµ‹ä¼˜åŒ–
        args = {
            'data': self.config['dataset']['dataset_yaml'],
            'epochs': training_config.get('epochs', 100),
            'batch': training_config.get('batch_size', 16),
            'imgsz': training_config.get('image_size', 640),  # ç«ç‚¹æ£€æµ‹é€šå¸¸ä¸éœ€è¦ç‰¹åˆ«å¤§çš„å›¾åƒå°ºå¯¸
            'lr0': training_config.get('learning_rate', 0.01),
            'device': training_config.get('device', '0' if self._has_cuda() else 'cpu'),
            'workers': training_config.get('workers', 8),
            'name': training_config.get('experiment_name', 'fire_detection_experiment'),
            'save_period': training_config.get('save_period', -1),
            'cache': training_config.get('cache', 'ram'),
            'exist_ok': training_config.get('exist_ok', False),
            'resume': training_config.get('resume', False),
            'verbose': training_config.get('verbose', True),
            'patience': training_config.get('patience', 50),
            'plots': training_config.get('plots', True),
            'rect': training_config.get('rect', False),
            'optimizer': training_config.get('optimizer', 'SGD'),  # SGDé€šå¸¸å¯¹ç«ç‚¹æ£€æµ‹æ•ˆæœæ›´å¥½
            'val': training_config.get('val', True),
            'save_json': training_config.get('save_json', False),
            'freeze': training_config.get('freeze', False),
            'multi_scale': training_config.get('multi_scale', True),  # å¤šå°ºåº¦è®­ç»ƒå¯¹ç«ç‚¹æ£€æµ‹å¾ˆé‡è¦

            # è¶…å‚æ•° - é’ˆå¯¹ç«ç‚¹æ£€æµ‹ä¼˜åŒ–
            'lrf': self.config['training'].get('lrf', 0.01),
            'momentum': self.config['training'].get('momentum', 0.937),
            'weight_decay': self.config['training'].get('weight_decay', 0.0005),
            'warmup_epochs': self.config['training'].get('warmup_epochs', 3.0),
            'warmup_momentum': self.config['training'].get('warmup_momentum', 0.8),
            'warmup_bias_lr': self.config['training'].get('warmup_bias_lr', 0.1),
            'box': self.config['training'].get('box_loss_gain', 7.5),
            'cls': self.config['training'].get('cls_loss_gain', 0.5),
            'kobj': self.config['training'].get('obj_positive_weight', 1.0),
            'iou': self.config['training'].get('iou_threshold', 0.2),
        }

        # æ•°æ®å¢å¼ºå‚æ•° - é’ˆå¯¹ç«ç‚¹æ£€æµ‹çš„åˆç†å¢å¼º
        if 'augmentation' in training_config:
            aug_config = training_config['augmentation']
            args.update({
                'hsv_h': aug_config.get('hsv_h', 0.015),  # ç«ç‚¹è‰²è°ƒå˜åŒ–è¾ƒå°
                'hsv_s': aug_config.get('hsv_s', 0.7),    # é¥±å’Œåº¦å¯é€‚åº¦å˜åŒ–
                'hsv_v': aug_config.get('hsv_v', 0.4),    # äº®åº¦å˜åŒ–å¯¹ç«ç‚¹æ£€æµ‹é‡è¦
                'degrees': aug_config.get('degrees', 10.0),    # ç«ç‚¹æ£€æµ‹å¯æ¥å—æ›´å¤§æ—‹è½¬
                'translate': aug_config.get('translate', 0.1),  # å¹³ç§»
                'scale': aug_config.get('scale', 0.5),         # ç¼©æ”¾
                'shear': aug_config.get('shear', 0.0),         # å‰ªåˆ‡
                'perspective': aug_config.get('perspective', 0.0),  # é€è§†å˜æ¢
                'flipud': aug_config.get('flipud', 0.5),       # ä¸Šä¸‹ç¿»è½¬
                'fliplr': aug_config.get('fliplr', 0.5),       # å·¦å³ç¿»è½¬
                'mosaic': aug_config.get('mosaic', 1.0),       # Mosaicå¢å¼º
                'mixup': aug_config.get('mixup', 0.0),         # ç«ç‚¹æ£€æµ‹ä¸å»ºè®®ä½¿ç”¨mixup
            })

        return args

    def setup_class_specific_params(self) -> Dict[str, Any]:
        """è®¾ç½®ç±»åˆ«ç‰¹å®šçš„å¼ºåŒ–å‚æ•°"""
        class_specific = self.config.get('class_specific', {})

        if not class_specific:
            return {}

        # éªŒè¯ç±»åˆ«IDæ˜¯å¦æœ‰æ•ˆ
        valid_class_params = {}
        for class_id, params in class_specific.items():
            try:
                class_idx = int(class_id)
                if class_idx >= self.num_classes:
                    logger.warning(f"ç±»åˆ«ID {class_idx} è¶…å‡ºèŒƒå›´(0-{self.num_classes-1})ï¼Œè·³è¿‡")
                    continue
                valid_class_params[class_idx] = params
                logger.info(f"å¯ç”¨ç±»åˆ« {class_idx} çš„å¼ºåŒ–å‚æ•°: {params}")
            except ValueError:
                logger.warning(f"æ— æ•ˆçš„ç±»åˆ«ID: {class_id}ï¼Œè·³è¿‡")

        return valid_class_params

    def _has_cuda(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰CUDAæ”¯æŒ"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False

    def create_custom_hyp(self) -> Dict[str, Any]:
        """åˆ›å»ºè‡ªå®šä¹‰è¶…å‚æ•°å­—å…¸"""
        return {}

    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        logger.info("å¼€å§‹æ”¯æŒè´Ÿæ ·æœ¬çš„YOLO11/12ç«ç‚¹æ£€æµ‹è®­ç»ƒ...")

        # é¦–å…ˆéªŒè¯æ•°æ®é›†ç»“æ„
        self.validate_dataset_structure()

        # åŠ è½½æ¨¡å‹
        model_name = self.get_model_name()
        logger.info(f"åŠ è½½æ¨¡å‹: {model_name}")
        model = YOLO(model_name)

        # è®¾ç½®è®­ç»ƒå‚æ•°
        training_args = self.setup_training_args()

        logger.info("è®­ç»ƒå‚æ•°:")
        for key, value in training_args.items():
            logger.info(f"  {key}: {value}")

        # ä¿å­˜é…ç½®åˆ°è®­ç»ƒç»“æœç›®å½•
        save_dir = Path("runs/detect") / training_args['name']
        save_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜è®­ç»ƒé…ç½®
        config_save_path = save_dir / "train_config.yaml"
        with open(config_save_path, 'w', encoding='utf-8') as f:
            yaml.dump(self.config, f, default_flow_style=False, allow_unicode=True)
        logger.info(f"è®­ç»ƒé…ç½®å·²ä¿å­˜åˆ°: {config_save_path}")

        # å¼€å§‹è®­ç»ƒ
        try:
            results = model.train(**training_args)

            logger.info("è®­ç»ƒå®Œæˆ!")
            logger.info(f"æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {results.save_dir / 'weights' / 'best.pt'}")

            # è¯„ä¼°æ¨¡å‹
            if self.config['training'].get('evaluate', True):
                logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
                metrics = model.val(data=self.config['dataset']['dataset_yaml'])
                logger.info(f"mAP50: {metrics.box.map50:.4f}")
                logger.info(f"mAP50-95: {metrics.box.map:.4f}")

            return results

        except KeyboardInterrupt:
            logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
            logger.info("æ£€æŸ¥æ˜¯å¦ä¿å­˜äº†éƒ¨åˆ†è®­ç»ƒç»“æœ...")
            return None

        except Exception as e:
            logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    import signal
    import os
    import threading
    import time

    def signal_handler(signum, frame):
        logger.info(f"æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å· {signum}ï¼Œæ­£åœ¨æ¸…ç†...")

        # å¼ºåˆ¶ç»ˆæ­¢æ‰€æœ‰ç›¸å…³è¿›ç¨‹
        logger.info("æ­£åœ¨ç»ˆæ­¢Ultralyticsè¿›ç¨‹...")

        # æ–¹æ³•1: ç»ˆæ­¢å½“å‰è¿›ç¨‹ç»„
        try:
            os.killpg(os.getpgrp(), signal.SIGTERM)
        except:
            pass

        # æ–¹æ³•2: æŸ¥æ‰¾å¹¶ç»ˆæ­¢Ultralyticsç›¸å…³è¿›ç¨‹
        try:
            import subprocess
            subprocess.run(['pkill', '-f', 'Ultralytics'], timeout=5)
            subprocess.run(['pkill', '-f', '_temp_'], timeout=5)
        except:
            pass

        # ç­‰å¾…2ç§’åå¼ºåˆ¶é€€å‡º
        def delayed_exit():
            time.sleep(2)
            logger.info("å¼ºåˆ¶é€€å‡º...")
            os._exit(1)

        thread = threading.Thread(target=delayed_exit, daemon=True)
        thread.start()

        sys.exit(0)

    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # è®¾ç½®è¿›ç¨‹ç»„ï¼Œä¾¿äºä¿¡å·ä¼ æ’­
    try:
        os.setpgrp()
    except:
        pass

    parser = argparse.ArgumentParser(description='æ”¯æŒè´Ÿæ ·æœ¬çš„YOLO11/12ç«ç‚¹æ£€æµ‹è®­ç»ƒç¨‹åº')
    parser.add_argument('--config', type=str, default='train_config_model2.yaml',
                       help='è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', action='store_true',
                       help='æ¢å¤è®­ç»ƒ')
    parser.add_argument('--device', type=str, default=0,
                       help='æŒ‡å®šè®¾å¤‡ï¼Œå¦‚0,1,2,3æˆ–cpu')

    args = parser.parse_args()

    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.config):
        print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        print("è¯·åˆ›å»ºtrain_config_model2.yamlæ–‡ä»¶æˆ–ä½¿ç”¨--configæŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„")
        return

    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = YOLONegativeTrainer(args.config)

        # å¦‚æœæŒ‡å®šäº†æ¢å¤è®­ç»ƒ
        if args.resume:
            trainer.config['training']['resume'] = True

        # å¦‚æœæŒ‡å®šäº†è®¾å¤‡
        if args.device:
            trainer.config['training']['device'] = args.device

        # å¼€å§‹è®­ç»ƒ
        trainer.train()

    except Exception as e:
        logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
        return


if __name__ == "__main__":
    main()